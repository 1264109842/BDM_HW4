# -*- coding: utf-8 -*-
"""BDM_HW4_df.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fy1zulJFyWjmfgpS1zAHzkcLtKiiXoxX
"""

from pyspark import SparkContext
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql import types as T
import datetime
import json
import numpy as np
import sys

def main(sc, spark):
    '''
    Transfer our code from the notebook here, however, remember to replace
    the file paths with the ones provided in the problem description.
    '''
    dfPlaces = spark.read.csv('/data/share/bdm/core-places-nyc.csv', header=True, escape='"')
    dfPattern = spark.read.csv('/data/share/bdm/weekly-patterns-nyc-2019-2020/*', header=True, escape='"')
    OUTPUT_PREFIX = sys.argv[1]
    
    def setNaics(n):
      for a, b in enumerate(NAICS.values()):
        if n in b:
          return a

    NAICS = {"/big_box_grocers" : {'452210','452311'},
          "/convenience_stores" : {'445120'},
          "/drinking_places" : {'722410'},
        "/full_service_restaurants" : {'722511'}, 
        "/limited_service_restaurants" : {'722513'}, 
        "/pharmacies_and_drug_stores" : {'446110','446191'},
        "/snack_and_bakeries" : {'311811','722515'},
        "/specialty_food_stores" : {'445210','445220','445230','445291','445292','445299'},
        "/supermarkets_except_convenience_stores" : {'445110'}}

    files = ["/big_box_grocers", "/convenience_stores", "/drinking_places",
            "/full_service_restaurants", "/limited_service_restaurants", "/pharmacies_and_drug_stores",
            "/snack_and_bakeries", "/specialty_food_stores", "/supermarkets_except_convenience_stores"]

    naics = set.union(*NAICS.values())

    udfNaics  = F.udf(setNaics, T.IntegerType())

    df = dfPlaces.filter(F.col('naics_code').isin(*naics))\
      .select('placeKey', udfNaics('naics_code').alias('Group')).cache()

    groupCount = dict(df.groupBy('group')\
              .count()\
              .collect())

    def mapday(s, v):
      date_1 = datetime.datetime.strptime(s[:10], '%Y-%m-%d')
      result = {}

      l = json.loads(v)

      for i in range(0,7):
        if l[i] == 0: continue
        date = date_1 + datetime.timedelta(days=i)
        if date.year in (2019, 2020):
          result[date] = l[i]

      return result

    def median(group,values_list):
      values_list = np.fromiter(values_list, np.int_)
      values_list.resize(groupCount[group])
      med = np.median(values_list)
      stdev = np.std(values_list)
      return [int(med), max(0, int(med-stdev+0.5)), int(med+stdev+0.5)]

    udfExpand = F.udf(mapday, T.MapType(T.DateType(), T.IntegerType()))
    udfMedian = F.udf(median, T.ArrayType(T.IntegerType()))

    newDF = dfPattern.join(F.broadcast(df), (dfPattern.placekey == df.placeKey))\
                  .select('Group', F.explode(udfExpand('date_range_start', 'visits_by_day')).alias('date', 'visits')).cache()
    
    newDFF = newDF.groupBy('Group','date')\
            .agg(F.collect_list('visits').alias('visits'))\
            .withColumn('median', udfMedian('Group', 'visits'))\
            .withColumn('year', F.substring('date',1,4))\
            .withColumn('date', F.regexp_replace('date', '2019', '2020'))\
            .orderBy('year','date')\
            .coalesce(1)\
            .cache()

    for i, j in enumerate(NAICS):
      newDFF.filter(F.col('Group') == i)\
            .select('year', 'date', newDFF.median[0].alias('median'), newDFF.median[1].alias('low'), newDFF.median[2].alias('high'))\
            .write.csv(OUTPUT_PREFIX+list(NAICS.keys())[i], mode='overwrite', header=True)

if __name__=='__main__':
    sc = SparkContext()
    spark = SparkSession(sc)
    main(sc, spark)