{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BDM_HW4_You.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOXqQWqyIYqxwJpo6EAhLaU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1264109842/BDM_HW4/blob/main/BDM_HW4_You.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoAr2U9-jKhd"
      },
      "source": [
        "import pyspark\n",
        "import sys\n",
        "import json\n",
        "import csv\n",
        "from datetime import datetime\n",
        "from datetime import timedelta  \n",
        "import sys\n",
        "import statistics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JSzSjJ_jom-"
      },
      "source": [
        "def strTolist(x):\n",
        "  ans = []\n",
        "  temp = ''\n",
        "\n",
        "  for i in range(0, len(x)):\n",
        "    if x[i].isdigit():\n",
        "      temp = temp + x[i]\n",
        "    elif not x[i].isdigit() and temp:\n",
        "      ans.append(int(temp))\n",
        "      temp = ''\n",
        "\n",
        "  return ans\n",
        "\n",
        "def mapday(s, v):\n",
        "  date_1 = datetime.strptime(s, '%Y-%m-%d')\n",
        "\n",
        "  result = ()\n",
        "\n",
        "  for i in range(0,7):\n",
        "    date = date_1 + timedelta(days=i)\n",
        "    result += (str(date)[:10], v[i]),\n",
        "\n",
        "  return result\n",
        "\n",
        "def low(x, y):\n",
        "  diff = x-statistics.pstdev(y)\n",
        "  if diff < 0:\n",
        "    return 0\n",
        "  else:\n",
        "    return int(round(diff))\n",
        "\n",
        "def high(x,y):\n",
        "  diff = x+statistics.pstdev(y)\n",
        "  if diff < 0:\n",
        "    return 0\n",
        "  else:\n",
        "    return int(round(diff))\n",
        "\n",
        "if __name__=='__main__':\n",
        "\n",
        "  NAICS = [['452210','452311'],['445120'],['722410'],['722511'],['722513'],['446110','446191'],['311811','722515'],['445210','445220','445230','445291','445292','445299'],['445110']]\n",
        "  files = [\"test/big_box_grocers\", \"test/convenience_stores\", \"test/drinking_places\", \"test/full_service_restaurants\", \"test/limited_service_restaurants\", \"test/pharmacies_and_drug_stores\",\n",
        "         \"test/snack_and_bakeries\", \"test/specialty_food_stores\", \"test/supermarkets_except_convenience_stores\"]\n",
        "\n",
        "  sc = pyspark.SparkContext()\n",
        "\n",
        "  data = []\n",
        "  category = []\n",
        "\n",
        "  for i in range(len(NAICS)):\n",
        "    data.append(sc.textFile('hdfs:///data/share/bdm/core-places-nyc.csv')\\\n",
        "                  .filter(lambda x: next(csv.reader([x]))[9] in NAICS[i])\\\n",
        "                  .cache()\n",
        "              )\n",
        "    category.append(data[i] \\\n",
        "                    .map(lambda x: next(csv.reader([x])))\\\n",
        "                    .map(lambda x: (x[0],x[1]))\n",
        "                  )\n",
        "    \n",
        "  new_category = []\n",
        "  for i in range(len(category)):\n",
        "    new_category.append(category[i].collect())\n",
        "\n",
        "  new_data = []\n",
        "  weekly_data = []\n",
        "\n",
        "  for i in range(len(NAICS)):\n",
        "    new_data.append(sc.textFile('hdfs:///data/share/bdm/weekly-patterns-nyc-2019-2020/*') \\\n",
        "                      .filter(lambda x: tuple(next(csv.reader([x]))[0:2]) in new_category[i])\\\n",
        "                      .cache()\n",
        "                  )\n",
        "    weekly_data.append(new_data[i]\\\n",
        "                      .map(lambda x: next(csv.reader([x])))\\\n",
        "                      .map(lambda x: (x[12][:10],strTolist(x[16])))\n",
        "                      )\n",
        "\n",
        "  for i in range(0,len(NAICS)):\n",
        "    weekly_data[i]\\\n",
        "      .map(lambda x : mapday(x[0],x[1]))\\\n",
        "      .flatMap(lambda x: x) \\\n",
        "      .filter(lambda x: x[1] != 0 and x[0] > '2018-12-31' and x[0] < '2021-01-01')\\\n",
        "      .groupByKey() \\\n",
        "      .mapValues(list)\\\n",
        "      .sortBy(lambda x: x[0])\\\n",
        "      .map(lambda x: (x[0],sorted(x[1])))\\\n",
        "      .map(lambda x: (x[0][:4], x[0], int(round(statistics.median(x[1]))), low(statistics.median(x[1]), x[1]), high(statistics.median(x[1]), x[1])))\n",
        "      .saveAsTextFile(files[i])\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}