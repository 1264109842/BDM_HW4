# -*- coding: utf-8 -*-
"""Copy of BDM_HW4_You.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jGwVKQpU5SmQxANK3m2FUzgrcPdX99bV
"""

# !pip install pyspark

import pyspark
import sys
import json
import csv
from datetime import datetime
from datetime import timedelta  
import sys
from pyspark.sql import SparkSession
from pyspark import SparkContext
import numpy as np


sc = pyspark.SparkContext()
spark = SparkSession(sc)

def mapday(s, v):
  date_1 = datetime.strptime(s, '%Y-%m-%d')

  result = ()

  for i in range(0,7):
    date = date_1 + timedelta(days=i)
    result += (str(date)[:10], v[i]),

  return result

def low(x, y):
  diff = int(round(x-y))
  if diff < 0:
    return 0
  else:
    return diff

def high(x,y):
  diff = int(round(x+y))
  if diff < 0:
    return 0
  else:
    return diff



if __name__=='__main__':

  NAICS = [['452210','452311'],['445120'],['722410'],
         ['722511'],['722513'],['446110','446191'],['311811','722515'],
         ['445210','445220','445230','445291','445292','445299'],['445110']]
  files = ["test/big_box_grocers", "test/convenience_stores", "test/drinking_places", 
          "test/full_service_restaurants", "test/limited_service_restaurants", "test/pharmacies_and_drug_stores",
          "test/snack_and_bakeries", "test/specialty_food_stores", "test/supermarkets_except_convenience_stores"]

  data = []
  new_data = []

  # for i in range(len(NAICS)):
  data.append(sc.textFile('hdfs:///data/share/bdm/core-places-nyc.csv')\
                .map(lambda x: next(csv.reader([x])))\
                .filter(lambda x: x[9] in NAICS[0])\
                .map(lambda x: [x[0],x[1]])\
                .cache()\
                .collect()
            )
  new_data.append(sc.textFile('hdfs:///data/share/bdm/weekly-patterns-nyc-2019-2020/*') \
                    .map(lambda x: next(csv.reader([x])))\
                    .filter(lambda x: x[0:2] in data[0])\
                    .map(lambda x: (x[12][:10],x[16]))\
                    .flatMap(lambda x : mapday(x[0],json.loads(x[1])))\
                    .filter(lambda x: x[1] > 0 and x[0] > '2018-12-31' and x[0] < '2021-01-01')\
                    .groupByKey() \
                    .mapValues(list)\
                    .sortBy(lambda x: x[0])\
                    .map(lambda x: (x[0][:4], "2020"+x[0][4:], np.median(x[1]), np.std(x[1])))
                )

  new_data[0].map(lambda x: (x[0], x[1], int(round(x[2])), low(x[2], x[3]), high(x[2], x[3])))\
                .map(lambda x: (x[0],x[1],x[2],x[3],x[4]))\
                .toDF(["year","date","median","low","high"])\
                .coalesce(1)\
                .write.option("header","true")\
                .csv(files[0])

new_data[0].collect()

# def mapday(s, ss, v):
#   date_1 = datetime.strptime(s, '%Y-%m-%d')
#   date_2 = datetime.strptime(ss, '%Y-%m-%d')
#   date_3 = (date_2 - date_1).days
#   result = ()

#   for i in range(0,date_3):
#     date = date_1 + timedelta(days=i)
#     result += (str(date)[:10], v[i]),

#   return result

# print(mapday('2020-11-29', '2020-12-05', [1,2,3,4,5,6,7]))