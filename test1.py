# -*- coding: utf-8 -*-
"""test1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H4BNb7Ujw0n6mwepH6SieqegyM-rN1dv
"""

import pyspark
import sys
import json
import csv
from datetime import datetime
from datetime import timedelta  
import sys
from pyspark.sql import SparkSession
from pyspark import SparkContext
import numpy as np
from pyspark.sql import functions as F
from pyspark.sql.types import DateType, IntegerType, MapType, StringType, FloatType
from pyspark.sql.functions import split, col, substring, regexp_replace, explode


sc = pyspark.SparkContext()
spark = SparkSession(sc)

def mapday(s, v):
  date_1 = datetime.strptime(s[:10], '%Y-%m-%d')
  result = ()

  for i in range(0,7):
    date = date_1 + timedelta(days=i)
    result += (str(date)[:10], v[i]),

  return result

def low(x, y):
  diff = int(round(x-y))
  return 0 if diff < 0 else diff

def high(x,y):
  diff = int(round(x+y))
  return 0 if diff < 0 else diff

if __name__=='__main__':
  NAICS = [['452210','452311'],['445120'],['722410'],
        ['722511'],['722513'],['446110','446191'],['311811','722515'],
        ['445210','445220','445230','445291','445292','445299'],['445110']]

  files = ["test/big_box_grocers", "test/convenience_stores", "test/drinking_places", 
        "test/full_service_restaurants", "test/limited_service_restaurants", "test/pharmacies_and_drug_stores",
        "test/snack_and_bakeries", "test/specialty_food_stores", "test/supermarkets_except_convenience_stores"]

  data = []
  new_data = []

for i in range(len(NAICS)):
  data.append(sc.textFile('hdfs:///data/share/bdm/core-places-nyc.csv')\
                .map(lambda x: next(csv.reader([x])))\
                .filter(lambda x: x[9] in NAICS[i])\
                .map(lambda x: [x[0],x[1]])\
                .cache()\
                .collect()
            )

  new_data.append(sc.textFile('hdfs:///data/share/bdm/weekly-patterns-nyc-2019-2020/*') \
                    .map(lambda x: next(csv.reader([x])))\
                    .filter(lambda x: x[:2] in data[i])\
                    .flatMap(lambda x : mapday(x[12][:10],json.loads(x[16])))\
                    .filter(lambda x: x[1] > 0 and x[0] > '2018-12-31' and x[0] < '2021-01-01')\
                    .groupByKey() \
                    .mapValues(list)\
                    .sortBy(lambda x: x[0])\
                    .map(lambda x: (x[0][:4], "2020"+x[0][4:], np.median(x[1]), np.std(x[1])))\
                    .map(lambda x: (x[0], x[1], int(round(x[2])), low(x[2], x[3]), high(x[2], x[3])))\
                    .cache()
                )

  new_data[i].map(lambda x: (x[0],x[1],x[2],x[3],x[4]))\
              .toDF(["year","date","median","low","high"])\
              .coalesce(1)\
              .write.format("csv")\
              .option("header","true")\
              .save(files[i])